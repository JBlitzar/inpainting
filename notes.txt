Over-regularizing
underfitting
Due to small bottleneck, or adding a FC layer (the two things I added)
weird weight initialization, found a bad local minimum
overfitting on version without linears?
unable to overfit with linears?
validation set (80/20)
test vs val: Training is used to optimize your weights, no overlapping data in split
val is for tuning hyperparameters: Train with various hyperparameters, look at val accuracy, make sure not overfitting
test set is to see how well it generalizes: "hyperparameter overfitting"

Gold standard: make sure you arent cheating by checking the test set and then tweaking them after. 

K-fold cross validation
way to get more data, not data augmentation
you have 80/20, get your 80% and split it into k=5
fold 1 is val for folds 2,3,4,5 Train
fold 2 is val for folds 1,3,4,5 train

image centering: eyes centered, training is easier
try image augmentation: randomly rotate, resize, recolor, etc = more data, more variation, harder to learn, better generalization
https://pytorch.org/vision/stable/transforms.html
https://pytorch.org/vision/0.13/transforms.html

out-of-distribution image: neural networks dont know how to say "idk"
flipping bits until it gives a different result
gradient descent: too high learning rate, giant spike
spike could be a data issue, outliers, etc while training.
too low learning rate: gets stuck easily, takes a long time
next steps: trying with real-world images, augmentation, unet
partial convolution? (very challenging, I would have to recreate the paper and code the stuff myself) https://openaccess.thecvf.com/content_ECCV_2018/papers/Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper.pdf

Next step: data augmentation with cnn

find a research paper with code on github too: replication


Feb 6: 
Image augmentation has been successful with very small losses on both augmented and non-augmented data.
Very promising results on testing images.
Surprisingly, still bad results when generalizing to the real world
--
It was doing the same augmentation for all the images
Changed it to do different augmentation, learning is "harder" as expected
Coding class:

Attention mechanism see paper
Dialated convolution
Do dialations at higher layers
partial/gated convolution (?)

Unexpected oom errors when running it with a new dataloader -> was using 256x256 images instead of 128x128

Feb 8
--
https://www.desmos.com/calculator/hvv3elxnpm
Dialated convolutions actually *reduce* feature map size, add padding to account for it.
Implemented dialated convs.
Made the switch to 0-1 instead of 0-255 data, smaller loss values and better training. Had to wrangle the images
It's good to finally use a dataloader.

Notes while reading paper:
attention mechanisms
datasets: *imagenet, *celebahq, places2, paris street view
SNR error? The peak signal-to-noise ratio (PSNR) is based on the error between corresponding pixels and is used to evaluate the quality of an image in comparison to the true image. The value typically ranges between 20 and 40. A higher value indicates lower distortion
and better image quality. 

SSIM error The structural similarity index (SSIM) measures the structural similarity of two images, simulating human visual perception. A larger value suggests less image distortion. When two images are identical, the SSIM value is 1. The theory behind SSIM suggests that natural images possess a highly structured feature, meaning pixels have a strong correlation that carries important information about the structure of visual scenes. In its application, images are often divided into blocks, and the SSIM is calculated for each block before taking the average. Given two images (x and y), the formula to compute SSIM is:

French inception distance: looks promising!! https://pytorch.org/torcheval/main/generated/torcheval.metrics.FrechetInceptionDistance.html

someone coded it for me on pytorch https://github.com/bonlime/pytorch-tools/blob/master/pytorch_tools/metrics/psnr.py


Discriminator after reconstruction?
t'=Decoder(Encoder(T))
Loss between reconstructed and original, but also you can have adversarial loss
This is represented as Lad = maxp Ex∼X[log(D(e))] − Ez∼Z[log(1 − D(G(z)))], where D is the discriminator, G is the generator, X is the distribution of real images, and Z is the distribution of the latent space. 

Feb 10
Autoencoder, by nature, leads to loss of information through the bottleneck
I implemented U-net architecture with skip connections so that later layers in the decoder can see encoder data. 
With all skip connections, it had lots of detail but didnt properly fill in black rectangles -- I removed the skip connection on the very last layer

Feb 11:
Training with unet - Experienced weird instabilities. After ~10 epochs, loss jumps up and stays horizontal + images are perfectly rgb-black. Once, rgb-red (sus)
I implemented a "training wheels" feature that reverts the model if that happens to it -- sign of deeper problem. 
Trained well with small rectangles, retrained with bigger ones and performance improved further. 
